{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2019db12-1b55-498b-92d7-f6376037e908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded MFG_1_To_8999_SF_CSV.csv with shape (487, 51)\n",
      "✅ Loaded MFG_9000_To_19000_SF_CSV.csv with shape (490, 51)\n",
      "✅ Loaded MFG_40k_To_140K_SF_CSV.csv with shape (496, 51)\n",
      "✅ Loaded MFG_140k_To_2Milli_SF_CSV.csv with shape (170, 51)\n",
      "✅ Loaded MFG_19000_To_40000_SF_CSV.csv with shape (491, 51)\n",
      "\n",
      "✅ Combined 5 files into a DataFrame with 2134 rows and 51 columns.\n"
     ]
    }
   ],
   "source": [
    "# Using glob to merge our costar export data to get around 499 Record Limit\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get a list of all CSV files in the current directory\n",
    "csv_files = glob.glob('*.csv')\n",
    "\n",
    "# Loop and read each CSV and add it to a list of DataFrames\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "        print(f\"✅ Loaded {file} with shape {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to read {file}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Optional: Save the result to a new CSV file\n",
    "combined_df.to_csv('All_Costar_MFG_Exports_Combined.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Combined {len(csv_files)} files into a DataFrame with {combined_df.shape[0]} rows and {combined_df.shape[1]} columns.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05af805-88f8-4b48-8144-ebf56604f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(combined_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f551be5-3981-4a53-9c7a-bd88e3c6efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' from simple_salesforce import Salesforce\n",
    "# Replace these placeholders with your Salesforce credentials\n",
    "USERNAME = 'SF User'  # Your Salesforce username\n",
    "PASSWORD = 'SF Pass'  # Your Salesforce password\n",
    "SECURITY_TOKEN = 'ST'\n",
    "\n",
    "# Replace these with your credentials\n",
    "sf = Salesforce(USERNAME,PASSWORD,SECURITY_TOKEN,domain='login')\n",
    "\n",
    "# Test query\n",
    "query = \"\"\"\n",
    "SELECT Sub_Market_Name__c, ascendix__Property__r.Name, Property_Record_Type__c, \n",
    "Property_Sub_Type__c, Total_Land_Area_SF__c, ascendix__ListingPrice__c,\n",
    "Listing_Price_PSF__c, ascendix__ListingBrokerContact__r.Name, \n",
    "ascendix__ListingBrokerCompany__r.Name, ascendix__SaleDate__c, ascendix__Property__Costar_ID__c,\n",
    "FROM ascendix__Property__c\n",
    "\"\"\"\n",
    "\n",
    "#Execute into DF\n",
    "results = sf.query(query)\n",
    "\n",
    "print(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a9e62-6fda-41a7-82c6-d5f9339b87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # Get metadata for all accessible objects, checking our user permissions\n",
    "all_objects = sf.describe()['sobjects']\n",
    "\n",
    "# Filter for objects with \"Property\" in their name or label\n",
    "property_objects = [\n",
    "    (obj['name'], obj['label']) \n",
    "    for obj in all_objects \n",
    "    if 'property' in obj['name'].lower() or 'property' in obj['label'].lower()\n",
    "]\n",
    "\n",
    "# Display the results\n",
    "for api_name, label in property_objects:\n",
    "    print(f\"{label} → {api_name}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3fee555-b245-41a3-95be-46ac69f14cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading both exports with pandas and loading into csv so that we can merge on matching (PK) --> costar Id's\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load Ascendix data\n",
    "ascendix_df = pd.read_csv(r\"Ascendix_Where_Costar__ID_Not_Blank.csv\")\n",
    "\n",
    "# Load Costar data\n",
    "costar_df = pd.read_csv(r\"All_Costar_MFG_Exports_Combined.csv\")\n",
    "\n",
    "# Looking at df's\n",
    "ascendix_df.head()\n",
    "costar_df.head()\n",
    "\n",
    "\n",
    "# By default, Ascendix & Costar will normalize data differently ---> need to standardize\n",
    "costar_df[\"PropertyID\"] = costar_df[\"PropertyID\"].astype(str)\n",
    "ascendix_df[\"PropertyID\"] = ascendix_df[\"PropertyID\"].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133542e5-9031-4734-a97e-d992e89279c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that our df has proper column headers\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(costar_df.head())  # or df.head(100) if you want more rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef86c4-69a8-4cbe-824f-5b173c65dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to Standardize column names on PK C_ID\n",
    "\n",
    "ascendix_df.rename(columns={\"Costar\": \"PropertyID\"}, inplace=True)\n",
    "print(ascendix_df)\n",
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1785204-7c27-4bde-85f5-fc5c41d7fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on Costar ID\n",
    "# I want to isolate only the Costar Properties that are NOT found in Ascendix \n",
    "merged_df = costar_df.merge(ascendix_df, on=\"PropertyID\", how=\"left\", indicator=True)\n",
    "\n",
    "# Identify missing properties (PropertyID's that exist in Costar and not in Ascendix ---> 'left_only'\n",
    "missing_properties = merged_df[merged_df[\"_merge\"] == \"left_only\"]\n",
    "\n",
    "# Save missing properties to a NEW CSV so that they are all in one place for further investigation \n",
    "missing_properties.to_csv(\"missing_properties.csv\", index=False)\n",
    "\n",
    "# List all column names\n",
    "print(missing_properties.columns)\n",
    "# Print the number of rows in missing_properties\n",
    "print(f\"Number of rows: {len(missing_properties)}\")\n",
    "# Summary of the DataFrame\n",
    "missing_properties.info()\n",
    "# Print the shape of the DataFrame\n",
    "print(f\"Shape of missing_properties: {missing_properties.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3458424-30d1-46df-810a-311e0c1a760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move PK to front of DF to make more workable\n",
    "\n",
    "# Get the list of columns\n",
    "cols = list(missing_properties.columns)\n",
    "\n",
    "# Move 'PropertyID' to be after 'PropertyAddress'\n",
    "cols.remove(\"PropertyID\")  # Remove it from its current position\n",
    "index = cols.index(\"Property Address\") + 1  # Find position after 'PropertyAddress'\n",
    "cols.insert(index, \"PropertyID\")  # Insert it there\n",
    "\n",
    "# Reorder the DataFrame\n",
    "missing_properties = missing_properties[cols]\n",
    "\n",
    "# Double check change took effect\n",
    "\n",
    "print(missing_properties)\n",
    "missing_properties.to_csv(\"missing_properties_Final_V.csv\", index=False)\n",
    "'''missing_properties.to_csv(\"missing_properties.csv\", index=False)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5610484-decf-49d4-b58b-305f27c2bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = missing_properties[missing_properties.duplicated(subset=[\"PropertyID\"], keep=False)]\n",
    "print(f\"Shape of dupes: {duplicates.shape}\")\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d3ae8-8a78-4bfd-90d8-2e13bc4ae4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard coding ID's where crane > 1 as MFG into own column\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"MFG_missing_properties_in_Final_V.csv\")\n",
    "print(\"Completed Read\")\n",
    "print(missing_properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c0f73-a106-4b26-8fea-81bbe6b4ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype check\n",
    "\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb58da-46bf-468e-8e41-57ff5080affa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['missing_properties'] = np.where(df['Number Of Cranes'] >= 1, 'Yes', 'No') \n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afb0cd-ab3f-4405-b9ed-88c59704159c",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(missing_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848cf56-b7e6-4e6d-8f0c-4c547377f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1011 Costar ID Blank, 7166 Costar ID NOT Blank ---> Final Csv After Cleaning = 538 MFG Properties\n",
    "\n",
    "    538 \n",
    "\n",
    "IF crane > 1 we hardcode as mfg no matter what    ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c1223-4dd6-4be5-be3e-1a906fe943c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "import pandas as pd\n",
    "from simple_salesforce import Salesforce\n",
    "\n",
    "# Authenticate\n",
    "USERNAME = 'SF User'  # Your Salesforce username\n",
    "PASSWORD = 'SF Pass'  # Your Salesforce password\n",
    "SECURITY_TOKEN = 'ST'\n",
    "sf = Salesforce(username=USERNAME, password=PASSWORD, security_token=SECURITY_TOKEN)\n",
    "\n",
    "\n",
    "\n",
    "# First we need to load the missing_properties.csv into a Pandas DataFrame\n",
    "# missing_properties.csv MUST have headers that align with Salesforce field names\n",
    "# Batches?\n",
    "missing_properties_df = pd.read_csv(\"missing_properties.csv\")\n",
    "\n",
    "# Iterate through the DataFrame and add each property\n",
    "for index, row in missing_properties_df.iterrows():\n",
    "    try:\n",
    "        property_data = {\n",
    "            \"Record_Type__c\": row[\"Record Type\"],  # EXAMPLE FIELD MAPPINGS, NEED TO GET ALL Squared away before pushing fr fr\n",
    "            \"Property_Sub_Type__c\": row[\"Property Sub Type\"], # For every field that needs to be populated in Ascendix, find the Salesforce API field name and pair it with the matching header name from Cleaned CSV\n",
    "            \"Property_Name__c\": row[\"Property Name\"],\n",
    "            \"Location_Description__c\": row[\"Location Description\"],\n",
    "            \"Street__c\": row[\"Street\"],\n",
    "            \"City__c\": row[\"City\"],\n",
    "            \"State_Province__c\": row[\"State/Province\"],\n",
    "            \"Zip_Postal_Code__c\": row[\"Zip/Postal Code\"],\n",
    "            \"County__c\": row[\"County\"],\n",
    "            \"Geolocation_Coordinates__c\": row[\"Geolocation Coordinates\"],\n",
    "            \"Costar_ID__c\": row[\"Costar ID\"],\n",
    "            \"Tenancy__c\": row[\"Tenancy\"],\n",
    "            \"Building_Status__c\": row[\"Building Status\"],\n",
    "            \"Submarket__c\": row[\"Submarket\"],\n",
    "            \"Market__c\": row[\"Market\"],\n",
    "            \"Region__c\": row[\"Region\"],\n",
    "            \"Total_Building_Area_SF__c\": row[\"Total Building Area (SF)\"],\n",
    "            \"Office_Area_SF__c\": row[\"Office Area (SF)\"],\n",
    "            \"Total_Land_Area_Acres__c\": row[\"Total Land Area (Acres)\"],\n",
    "            \"Loading_Type__c\": row[\"Loading Type\"],\n",
    "            \"Docks__c\": row[\"Docks\"],\n",
    "            \"Drive_Ins__c\": row[\"Drive Ins\"],\n",
    "            \"Parking_Total__c\": row[\"Parking (Total)\"],\n",
    "            \"Listing_Broker_Company__c\": row[\"Listing Broker Company\"],\n",
    "            \"Listing_Broker_Contact__c\": row[\"Listing Broker Contact\"],\n",
    "        }\n",
    "            #Salesforce_API_names           #Header Names from CSV We cleaned from Costar on Right Side\n",
    "        # Add property to Ascendix\n",
    "        sf.Property__c.create(property_data)    #create method of the simple_salesforce library to add records programmatically.\n",
    "        print(f\"Successfully added property: {row['Property Name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding property: {row['Property Name']} - {e}\")\n",
    "\n",
    "\n",
    "# Need to add some sort of logging logic to handle failed inserts and make troubleshooting more straightforward\n",
    "''''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
